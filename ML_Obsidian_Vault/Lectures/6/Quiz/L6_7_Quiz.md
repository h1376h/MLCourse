# Lecture 6.7: Random Forest Quiz

## Overview
This quiz contains 5 questions covering different topics from section 6.7 of the lectures on Random Forest, including ensemble concept, bagging, feature randomization, and random forest advantages.

## Question 1

### Problem Statement
Random Forest is an ensemble method that combines multiple decision trees.

#### Task
1. [ğŸ”] What is the main principle behind Random Forest?
2. [ğŸ”] How does Random Forest differ from a single decision tree?
3. [ğŸ”] What is the relationship between Random Forest and bagging?
4. [ğŸ”] Why is Random Forest called "random"?

For a detailed explanation of this question, see [Question 1: Random Forest Overview](L6_7_1_explanation.md).

## Question 2

### Problem Statement
Random Forest uses bagging to create diverse trees.

#### Task
1. [ğŸ“š] How does bagging work in Random Forest?
2. [ğŸ“š] What is bootstrap sampling and why is it used?
3. [ğŸ“š] How many trees are typically used in a Random Forest?
4. [ğŸ“š] What happens if you use too few or too many trees?

For a detailed explanation of this question, see [Question 2: Bagging in Random Forest](L6_7_2_explanation.md).

## Question 3

### Problem Statement
Feature randomization is a key innovation in Random Forest.

#### Task
1. [ğŸ”] How does feature randomization work?
2. [ğŸ”] What is the purpose of feature randomization?
3. [ğŸ”] How do you choose the number of features to consider at each split?
4. [ğŸ”] What happens if you don't use feature randomization?

For a detailed explanation of this question, see [Question 3: Feature Randomization](L6_7_3_explanation.md).

## Question 4

### Problem Statement
Consider a Random Forest with the following parameters:

| Parameter | Value |
|-----------|-------|
| Number of trees | 100 |
| Max features per split | 3 |
| Bootstrap samples | Yes |
| Min samples per leaf | 5 |

#### Task
1. [ğŸ“š] How many different training datasets will be created?
2. [ğŸ“š] How many features will be considered at each split?
3. [ğŸ“š] What is the purpose of the min_samples_per_leaf parameter?
4. [ğŸ“š] How would you adjust these parameters for a larger dataset?

For a detailed explanation of this question, see [Question 4: Random Forest Parameters](L6_7_4_explanation.md).

## Question 5

### Problem Statement
Random Forest has several advantages over single decision trees.

#### Task
1. [ğŸ“š] **Advantage 1**: How does Random Forest handle overfitting?
2. [ğŸ“š] **Advantage 2**: Why is Random Forest more robust to noise?
3. [ğŸ“š] **Advantage 3**: How does Random Forest provide feature importance?
4. [ğŸ“š] **Advantage 4**: What makes Random Forest computationally efficient?

For a detailed explanation of this question, see [Question 5: Random Forest Advantages](L6_7_5_explanation.md).
