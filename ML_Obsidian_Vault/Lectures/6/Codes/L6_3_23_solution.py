import numpy as np
import matplotlib.pyplot as plt
import os
from matplotlib.patches import Rectangle, FancyBboxPatch
import matplotlib.patches as mpatches

# Create directory to save figures
script_dir = os.path.dirname(os.path.abspath(__file__))
images_dir = os.path.join(os.path.dirname(script_dir), "Images")
save_dir = os.path.join(images_dir, "L6_3_Quiz_23")
os.makedirs(save_dir, exist_ok=True)

# Enable LaTeX style plotting
plt.rcParams['text.usetex'] = True
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.size'] = 12

def print_step_header(step_number, step_title):
    """Print a formatted step header."""
    print(f"\n{'=' * 80}")
    print(f"STEP {step_number}: {step_title}")
    print(f"{'=' * 80}\n")

# Step 1: Understanding the Problem
print_step_header(1, "Understanding the Problem")

print("Question 23: Feature Selection Robustness Across Algorithms")
print("We need to analyze how different feature selection criteria (Information Gain vs Gain Ratio)")
print("handle noise features and identify the most robust algorithm.")
print()
print("Tasks:")
print("1. Calculate information gain for all three features")
print("2. Calculate gain ratio for all three features")
print("3. Which algorithm is most likely to select the relevant feature first?")
print("4. How do noise features affect each algorithm differently?")
print()

# Step 2: Given Dataset Analysis
print_step_header(2, "Given Dataset Analysis")

print("Dataset:")
print("| Relevant_Feature | Noise_Feature1 | Noise_Feature2 | Target |")
print("|------------------|----------------|----------------|--------|")
print("| A                | X              | 1              | Yes    |")
print("| A                | Y              | 2              | Yes    |")
print("| B                | Z              | 3              | No     |")
print("| B                | X              | 1              | No     |")
print("| A                | Z              | 2              | Yes    |")
print("| B                | Y              | 3              | No     |")
print()

print("Dataset Summary:")
print("- Total samples: 6")
print("- Relevant_Feature: A (3 samples), B (3 samples)")
print("- Noise_Feature1: X (2 samples), Y (2 samples), Z (2 samples)")
print("- Noise_Feature2: 1 (2 samples), 2 (2 samples), 3 (2 samples)")
print("- Target: Yes (3 samples), No (3 samples)")
print()

# Step 3: Understanding Information Gain and Gain Ratio
print_step_header(3, "Understanding Information Gain and Gain Ratio")

print("Information Gain (IG):")
print("IG(S, A) = H(S) - H(S|A)")
print("where:")
print("- H(S) is the entropy of the dataset")
print("- H(S|A) is the conditional entropy given feature A")
print("- Higher IG means better feature for splitting")
print()
print("Gain Ratio (GR):")
print("GR(S, A) = IG(S, A) / SplitInfo(A)")
print("where:")
print("- SplitInfo(A) measures the potential information generated by splitting on A")
print("- GR penalizes features with many values")
print("- Helps prevent bias towards features with many categories")
print()

# Step 4: Calculate Entropy of the Dataset
print_step_header(4, "Calculate Entropy of the Dataset")

print("Step 1: Calculate H(S) - Entropy of the entire dataset")
print("H(S) = -Σ p(i) * log2(p(i))")
print()

# Count target classes
total_samples = 6
yes_count = 3
no_count = 3

p_yes = yes_count / total_samples
p_no = no_count / total_samples

print(f"P(Yes) = {yes_count}/{total_samples} = {p_yes}")
print(f"P(No) = {no_count}/{total_samples} = {p_no}")
print()

# Calculate entropy
import math
H_S = -p_yes * math.log2(p_yes) - p_no * math.log2(p_no)
print(f"H(S) = -{p_yes} * log2({p_yes}) - {p_no} * log2({p_no})")
print(f"H(S) = -{p_yes} * {math.log2(p_yes):.3f} - {p_no} * {math.log2(p_no):.3f}")
print(f"H(S) = {H_S:.3f}")
print()

# Step 5: Calculate Information Gain for Each Feature
print_step_header(5, "Calculate Information Gain for Each Feature")

print("Step 2: Calculate Information Gain for each feature")
print("IG(S, A) = H(S) - H(S|A)")
print()

# Feature 1: Relevant_Feature
print("Feature 1: Relevant_Feature")
print("Values: A (3 samples), B (3 samples)")
print()

# For A: 3 samples, all Yes
print("For A = A:")
print("  - 3 samples, all Yes")
print("  - P(Yes|A=A) = 3/3 = 1.0")
print("  - P(No|A=A) = 0/3 = 0.0")
print("  - H(S|A=A) = -1.0 * log2(1.0) - 0.0 * log2(0.0) = 0")
print()

# For B: 3 samples, all No
print("For A = B:")
print("  - 3 samples, all No")
print("  - P(Yes|A=B) = 0/3 = 0.0")
print("  - P(No|A=B) = 3/3 = 1.0")
print("  - H(S|B) = -0.0 * log2(0.0) - 1.0 * log2(1.0) = 0")
print()

# Calculate conditional entropy
H_S_given_A = (3/6) * 0 + (3/6) * 0  # Weighted average
print(f"H(S|A) = (3/6) * 0 + (3/6) * 0 = {H_S_given_A:.3f}")
print()

# Calculate information gain
IG_A = H_S - H_S_given_A
print(f"IG(S, Relevant_Feature) = {H_S:.3f} - {H_S_given_A:.3f} = {IG_A:.3f}")
print()

# Feature 2: Noise_Feature1
print("Feature 2: Noise_Feature1")
print("Values: X (2 samples), Y (2 samples), Z (2 samples)")
print()

# For X: 2 samples, 1 Yes, 1 No
print("For Noise_Feature1 = X:")
print("  - 2 samples: 1 Yes, 1 No")
print("  - P(Yes|X) = 1/2 = 0.5")
print("  - P(No|X) = 1/2 = 0.5")
print("  - H(S|X) = -0.5 * log2(0.5) - 0.5 * log2(0.5) = 1.0")
print()

# For Y: 2 samples, 1 Yes, 1 No
print("For Noise_Feature1 = Y:")
print("  - 2 samples: 1 Yes, 1 No")
print("  - P(Yes|Y) = 1/2 = 0.5")
print("  - P(No|Y) = 1/2 = 0.5")
print("  - H(S|Y) = -0.5 * log2(0.5) - 0.5 * log2(0.5) = 1.0")
print()

# For Z: 2 samples, 1 Yes, 1 No
print("For Noise_Feature1 = Z:")
print("  - 2 samples: 1 Yes, 1 No")
print("  - P(Yes|Z) = 1/2 = 0.5")
print("  - P(No|Z) = 1/2 = 0.5")
print("  - H(S|Z) = -0.5 * log2(0.5) - 0.5 * log2(0.5) = 1.0")
print()

# Calculate conditional entropy
H_S_given_Noise1 = (2/6) * 1.0 + (2/6) * 1.0 + (2/6) * 1.0
print(f"H(S|Noise_Feature1) = (2/6) * 1.0 + (2/6) * 1.0 + (2/6) * 1.0 = {H_S_given_Noise1:.3f}")
print()

# Calculate information gain
IG_Noise1 = H_S - H_S_given_Noise1
print(f"IG(S, Noise_Feature1) = {H_S:.3f} - {H_S_given_Noise1:.3f} = {IG_Noise1:.3f}")
print()

# Feature 3: Noise_Feature2
print("Feature 3: Noise_Feature2")
print("Values: 1 (2 samples), 2 (2 samples), 3 (2 samples)")
print()

# For 1: 2 samples, 1 Yes, 1 No
print("For Noise_Feature2 = 1:")
print("  - 2 samples: 1 Yes, 1 No")
print("  - P(Yes|1) = 1/2 = 0.5")
print("  - P(No|1) = 1/2 = 0.5")
print("  - H(S|1) = -0.5 * log2(0.5) - 0.5 * log2(0.5) = 1.0")
print()

# For 2: 2 samples, 1 Yes, 1 No
print("For Noise_Feature2 = 2:")
print("  - 2 samples: 1 Yes, 1 No")
print("  - P(Yes|2) = 1/2 = 0.5")
print("  - P(No|2) = 1/2 = 0.5")
print("  - H(S|2) = -0.5 * log2(0.5) - 0.5 * log2(0.5) = 1.0")
print()

# For 3: 2 samples, 1 Yes, 1 No
print("For Noise_Feature2 = 3:")
print("  - 2 samples: 1 Yes, 1 No")
print("  - P(Yes|3) = 1/2 = 0.5")
print("  - P(No|3) = 1/2 = 0.5")
print("  - H(S|3) = -0.5 * log2(0.5) - 0.5 * log2(0.5) = 1.0")
print()

# Calculate conditional entropy
H_S_given_Noise2 = (2/6) * 1.0 + (2/6) * 1.0 + (2/6) * 1.0
print(f"H(S|Noise_Feature2) = (2/6) * 1.0 + (2/6) * 1.0 + (2/6) * 1.0 = {H_S_given_Noise2:.3f}")
print()

# Calculate information gain
IG_Noise2 = H_S - H_S_given_Noise2
print(f"IG(S, Noise_Feature2) = {H_S:.3f} - {H_S_given_Noise2:.3f} = {IG_Noise2:.3f}")
print()

# Step 6: Calculate Gain Ratio for Each Feature
print_step_header(6, "Calculate Gain Ratio for Each Feature")

print("Step 3: Calculate Gain Ratio for each feature")
print("GR(S, A) = IG(S, A) / SplitInfo(A)")
print()

# SplitInfo calculation
print("SplitInfo(A) = -Σ |Sv|/|S| * log2(|Sv|/|S|)")
print("where |Sv| is the number of samples with value v for feature A")
print()

# SplitInfo for Relevant_Feature
print("SplitInfo(Relevant_Feature):")
print("  - A: 3 samples, P(A) = 3/6 = 0.5")
print("  - B: 3 samples, P(B) = 3/6 = 0.5")
print("  - SplitInfo = -0.5 * log2(0.5) - 0.5 * log2(0.5)")
print(f"  - SplitInfo = -0.5 * {math.log2(0.5):.3f} - 0.5 * {math.log2(0.5):.3f}")
splitinfo_A = -0.5 * math.log2(0.5) - 0.5 * math.log2(0.5)
print(f"  - SplitInfo = {splitinfo_A:.3f}")
print()

# SplitInfo for Noise_Feature1
print("SplitInfo(Noise_Feature1):")
print("  - X: 2 samples, P(X) = 2/6 = 0.333")
print("  - Y: 2 samples, P(Y) = 2/6 = 0.333")
print("  - Z: 2 samples, P(Z) = 2/6 = 0.333")
print("  - SplitInfo = -0.333 * log2(0.333) - 0.333 * log2(0.333) - 0.333 * log2(0.333)")
splitinfo_Noise1 = -3 * (2/6) * math.log2(2/6)
print(f"  - SplitInfo = {splitinfo_Noise1:.3f}")
print()

# SplitInfo for Noise_Feature2
print("SplitInfo(Noise_Feature2):")
print("  - 1: 2 samples, P(1) = 2/6 = 0.333")
print("  - 2: 2 samples, P(2) = 2/6 = 0.333")
print("  - 3: 2 samples, P(3) = 2/6 = 0.333")
print("  - SplitInfo = -0.333 * log2(0.333) - 0.333 * log2(0.333) - 0.333 * log2(0.333)")
splitinfo_Noise2 = -3 * (2/6) * math.log2(2/6)
print(f"  - SplitInfo = {splitinfo_Noise2:.3f}")
print()

# Calculate Gain Ratios
print("Now calculate Gain Ratios:")
print()

GR_A = IG_A / splitinfo_A
print(f"GR(S, Relevant_Feature) = {IG_A:.3f} / {splitinfo_A:.3f} = {GR_A:.3f}")
print()

GR_Noise1 = IG_Noise1 / splitinfo_Noise1
print(f"GR(S, Noise_Feature1) = {IG_Noise1:.3f} / {splitinfo_Noise1:.3f} = {GR_Noise1:.3f}")
print()

GR_Noise2 = IG_Noise2 / splitinfo_Noise2
print(f"GR(S, Noise_Feature2) = {IG_Noise2:.3f} / {splitinfo_Noise2:.3f} = {GR_Noise2:.3f}")
print()

# Step 7: Results Summary and Comparison
print_step_header(7, "Results Summary and Comparison")

print("Information Gain Results:")
print("-" * 50)
print(f"{'Feature':<20} {'Information Gain':<15}")
print("-" * 50)
print(f"{'Relevant_Feature':<20} {IG_A:<15.3f}")
print(f"{'Noise_Feature1':<20} {IG_Noise1:<15.3f}")
print(f"{'Noise_Feature2':<20} {IG_Noise2:<15.3f}")
print("-" * 50)
print()

print("Gain Ratio Results:")
print("-" * 50)
print(f"{'Feature':<20} {'Gain Ratio':<15}")
print("-" * 50)
print(f"{'Relevant_Feature':<20} {GR_A:<15.3f}")
print(f"{'Noise_Feature1':<20} {GR_Noise1:<15.3f}")
print(f"{'Noise_Feature2':<20} {GR_Noise2:<15.3f}")
print("-" * 50)
print()

# Step 8: Algorithm Selection Analysis
print_step_header(8, "Algorithm Selection Analysis")

print("Question 3: Which algorithm is most likely to select the relevant feature first?")
print()

print("Analysis:")
print("1. Information Gain (ID3):")
print(f"   - Relevant_Feature: {IG_A:.3f}")
print(f"   - Noise_Feature1: {IG_Noise1:.3f}")
print(f"   - Noise_Feature2: {IG_Noise2:.3f}")
print("   - ID3 would select Relevant_Feature (highest IG)")
print()
print("2. Gain Ratio (C4.5):")
print(f"   - Relevant_Feature: {GR_A:.3f}")
print(f"   - Noise_Feature1: {GR_Noise1:.3f}")
print(f"   - Noise_Feature2: {GR_Noise2:.3f}")
print("   - C4.5 would also select Relevant_Feature (highest GR)")
print()

print("Answer: Both ID3 and C4.5 would select the Relevant_Feature first")
print("because it has the highest information gain and gain ratio.")
print()

# Step 9: Noise Feature Impact Analysis
print_step_header(9, "Noise Feature Impact Analysis")

print("Question 4: How do noise features affect each algorithm differently?")
print()

print("Impact on Information Gain (ID3):")
print(f"- Noise features have equal IG values: {IG_Noise1:.3f}")
print("- This is because they provide no useful information for classification")
print("- However, they don't get penalized for having many values")
print("- ID3 might be biased towards features with many categories")
print()
print("Impact on Gain Ratio (C4.5):")
print(f"- Noise features have equal GR values: {GR_Noise1:.3f}")
print("- Gain Ratio penalizes features with many values")
print("- This helps C4.5 be more robust against noise features")
print("- The penalty is more severe for features with more categories")
print()

# Step 10: Visualizing Information Gain Results
print_step_header(10, "Visualizing Information Gain Results")

fig, ax = plt.subplots(figsize=(10, 8))
features = [r'Relevant\\Feature', r'Noise\\Feature1', r'Noise\\Feature2']
ig_values = [IG_A, IG_Noise1, IG_Noise2]

bars = ax.bar(features, ig_values, color=['green', 'red', 'red'], alpha=0.7)
ax.set_ylabel(r'Information Gain', fontsize=14)
ax.set_title(r'Information Gain by Feature', fontsize=16, fontweight='bold')
ax.grid(True, alpha=0.3)

# Add value labels on bars
for bar, value in zip(bars, ig_values):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.savefig(os.path.join(save_dir, 'information_gain_comparison.png'), dpi=300, bbox_inches='tight')
plt.close()

# Step 11: Visualizing Gain Ratio Results
print_step_header(11, "Visualizing Gain Ratio Results")

fig, ax = plt.subplots(figsize=(10, 8))
gr_values = [GR_A, GR_Noise1, GR_Noise2]

bars = ax.bar(features, gr_values, color=['green', 'red', 'red'], alpha=0.7)
ax.set_ylabel(r'Gain Ratio', fontsize=14)
ax.set_title(r'Gain Ratio by Feature', fontsize=16, fontweight='bold')
ax.grid(True, alpha=0.3)

# Add value labels on bars
for bar, value in zip(bars, gr_values):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.savefig(os.path.join(save_dir, 'gain_ratio_comparison.png'), dpi=300, bbox_inches='tight')
plt.close()

# Step 12: Visualizing Side-by-Side Comparison
print_step_header(12, "Visualizing Side-by-Side Comparison")

fig, ax = plt.subplots(figsize=(12, 8))
x = np.arange(len(features))
width = 0.35

bars1 = ax.bar(x - width/2, ig_values, width, label=r'Information Gain', color='skyblue', alpha=0.7)
bars2 = ax.bar(x + width/2, gr_values, width, label=r'Gain Ratio', color='lightcoral', alpha=0.7)

ax.set_xlabel(r'Features', fontsize=14)
ax.set_ylabel(r'Score', fontsize=14)
ax.set_title(r'Information Gain vs Gain Ratio Comparison', fontsize=16, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(features)
ax.legend(fontsize=12)
ax.grid(True, alpha=0.3)

# Add value labels
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                 f'{height:.3f}', ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.savefig(os.path.join(save_dir, 'side_by_side_comparison.png'), dpi=300, bbox_inches='tight')
plt.close()

# Step 13: Visualizing Feature Selection Decision Tree
print_step_header(13, "Visualizing Feature Selection Decision Tree")

fig, ax = plt.subplots(figsize=(12, 8))
ax.set_xlim(0, 10)
ax.set_ylim(0, 10)
ax.axis('off')

# Draw the decision tree structure
def draw_node(x, y, text, color='lightblue', size=0.8):
    box = FancyBboxPatch(
        (x - size/2, y - size/2), size, size,
        boxstyle="round,pad=0.02",
        facecolor=color,
        edgecolor='black',
        linewidth=2
    )
    ax.add_patch(box)
    ax.text(x, y, text, ha='center', va='center', fontsize=9, fontweight='bold')

def draw_arrow(start, end, label='', color='black'):
    ax.annotate('', xy=end, xytext=start,
                arrowprops=dict(arrowstyle='->', color=color, lw=2))
    if label:
        mid_x = (start[0] + end[0]) / 2
        mid_y = (start[1] + end[1]) / 2
        ax.text(mid_x, mid_y, label, ha='center', va='center', 
                fontsize=8, bbox=dict(boxstyle="round,pad=0.2", fc="white", ec="black", alpha=0.8))

# Root node
draw_node(5, 9, 'Feature\nSelection', 'lightgreen', 1.2)

# Algorithm branches
draw_arrow((5, 8.4), (3, 7.5))
draw_arrow((5, 8.4), (7, 7.5))

draw_node(3, 7, 'ID3\n(Information Gain)', 'lightblue')
draw_node(7, 7, 'C4.5\n(Gain Ratio)', 'lightcoral')

# Feature selection for ID3
draw_arrow((3, 6.6), (1.5, 5.7))
draw_arrow((3, 6.6), (4.5, 5.7))

draw_node(1.5, 5, 'Relevant\nFeature\n(IG=1.000)', 'green')
draw_node(4.5, 5, 'Noise Features\n(IG=0.000)', 'red')

# Feature selection for C4.5
draw_arrow((7, 6.6), (5.5, 5.7))
draw_arrow((7, 6.6), (8.5, 5.7))

draw_node(5.5, 5, 'Relevant\nFeature\n(GR=1.000)', 'green')
draw_node(8.5, 5, 'Noise Features\n(GR=0.000)', 'red')

# Add explanation
ax.text(5, 2, r'Both algorithms select Relevant\_Feature first because it has the highest scores', 
        ha='center', va='center', fontsize=10, fontweight='bold',
        bbox=dict(boxstyle="round,pad=0.5", fc="lightyellow", ec="black", alpha=0.8))

plt.savefig(os.path.join(save_dir, 'feature_selection_decision_tree.png'), dpi=300, bbox_inches='tight')
plt.close()

# Step 14: Key Insights
print_step_header(14, "Key Insights")

print("1. Feature Selection Results:")
print("   - Relevant_Feature has perfect information gain and gain ratio")
print("   - Noise features have zero information gain and gain ratio")
print("   - Both algorithms correctly identify the relevant feature")
print()
print("2. Algorithm Robustness:")
print("   - In this case, both algorithms perform equally well")
print("   - The dataset is simple enough that noise doesn't confuse either algorithm")
print("   - Gain Ratio would show more benefit with more complex noise patterns")
print()
print("3. Noise Feature Impact:")
print("   - Noise features provide no useful information for classification")
print("   - They have equal impact on both algorithms in this simple case")
print("   - Gain Ratio provides better protection against high-cardinality noise features")
print()

# Step 15: Final Answer
print_step_header(15, "Final Answer")

print("1. Information Gain for all features:")
print(f"   - Relevant_Feature: {IG_A:.3f}")
print(f"   - Noise_Feature1: {IG_Noise1:.3f}")
print(f"   - Noise_Feature2: {IG_Noise2:.3f}")
print()
print("2. Gain Ratio for all features:")
print(f"   - Relevant_Feature: {GR_A:.3f}")
print(f"   - Noise_Feature1: {GR_Noise1:.3f}")
print(f"   - Noise_Feature2: {GR_Noise2:.3f}")
print()
print("3. Algorithm selection:")
print("   Both ID3 and C4.5 would select the Relevant_Feature first")
print("   because it has the highest scores in both metrics.")
print()
print("4. Noise feature impact:")
print("   - Noise features have zero information value")
print("   - Both algorithms are equally robust in this simple case")
print("   - Gain Ratio provides better protection against high-cardinality noise")
print()

print(f"\nVisualizations saved to: {save_dir}")
print("The plots show the feature selection results and algorithm comparison.")
print("Each plot is saved as a separate file for better clarity and use.")
