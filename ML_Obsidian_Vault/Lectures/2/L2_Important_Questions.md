# Lecture 2: Important Questions

This file collects references to important questions from the Lecture 2 quizzes, focusing on core concepts, fundamental derivations, and key applications in probability, statistics, and information theory for machine learning.

## From Lecture 2.1: Probability Fundamentals
- [[L2_1_Quiz#Question 1]] - Calculating basic probabilities and checking independence.
- [[L2_1_Quiz#Question 3]] - Working with continuous random variables (PDF, CDF, Expectation, Variance).
- [[L2_1_Quiz#Question 5]] - Applying Bayes' theorem in a practical scenario (medical diagnosis).
- [[L2_1_Quiz#Question 8]] - Understanding joint, marginal, and conditional distributions, and covariance.
- [[L2_1_Quiz#Question 13]] - Applying the Central Limit Theorem.
- [[L2_1_Quiz#Question 14]] - Using the normal distribution for modeling and calculations.
- [[L2_1_Quiz#Question 32]] - Classification using multivariate Gaussian distributions (discriminant functions, decision boundaries).

## From Lecture 2.2: Information Theory and Entropy
- [[L2_2_Quiz#Question 1]] - Calculating entropy and understanding its properties (e.g., maximum entropy).
- [[L2_2_Quiz#Question 2]] - Calculating KL Divergence and Cross-Entropy, understanding their relationship and asymmetry.
- [[L2_2_Quiz#Question 3]] - Calculating Joint Entropy and Mutual Information.
- [[L2_2_Quiz#Question 4]] - Understanding Cross-Entropy Loss in the context of classification.
- [[L2_2_Quiz#Question 7]] - Verifying the relationship between Entropy, Cross-Entropy, and KL Divergence.

## From Lecture 2.3: Statistical Estimation Basics
- [[L2_3_Quiz#Question 1]] - Constructing Likelihood and Log-Likelihood functions, finding MLE.
- [[L2_3_Quiz#Question 3]] - Evaluating estimator properties (bias, variance, efficiency, Cram√©r-Rao Lower Bound).
- [[L2_3_Quiz#Question 4]] - Calculating Mean Squared Error (MSE) and understanding the Bias-Variance Tradeoff.
- [[L2_3_Quiz#Question 5]] - Identifying and proving Sufficient Statistics using the factorization theorem.
- [[L2_3_Quiz#Question 6]] - Deriving and interpreting Fisher Information.
- [[L2_3_Quiz#Question 8]] - Distinguishing between Probability and Likelihood.

## From Lecture 2.4: Maximum Likelihood Estimation
- [[L2_4_Quiz#Question 2]] - Deriving MLE for the mean of a Normal distribution.
- [[L2_4_Quiz#Question 3]] - Deriving MLE for the parameter of a Bernoulli distribution and finding Fisher Information.
- [[L2_4_Quiz#Question 4]] - Deriving MLE for the rate parameter of an Exponential distribution and checking consistency.
- [[L2_4_Quiz#Question 6]] - Deriving MLE for the parameter of a Uniform distribution and understanding bias.
- [[L2_4_Quiz#Question 10]] - Using Likelihood Ratio Tests and AIC for model selection.
- [[L2_4_Quiz#Question 11]] - Deriving MLE for parameters in Linear Regression.
- [[L2_4_Quiz#Question 15]] - Relating Fisher Information, CRLB, and estimator efficiency for MLE.
- [[L2_4_Quiz#Question 16]] - Understanding asymptotic properties of MLE (Consistency, Asymptotic Normality, Efficiency).
- [[L2_4_Quiz#Question 27]] - Applying MLE concepts to Multinomial distributions and one-hot encoding in NLP.

## From Lecture 2.5: Bayesian Approach to ML
- [[L2_5_Quiz#Question 1]] - Performing a Bayesian update for a Binomial likelihood with a Beta prior (calculating posterior, mean, mode).
- [[L2_5_Quiz#Question 2]] - Applying Bayes' theorem iteratively in a practical scenario (medical diagnosis).
- [[L2_5_Quiz#Question 3]] - Deriving posterior and predictive distributions for a Poisson-Gamma model.
- [[L2_5_Quiz#Question 4]] - Performing Bayesian inference for the mean of a Normal distribution with a Normal prior.
- [[L2_5_Quiz#Question 5]] - Comparing the impact of informative vs. uninformative priors (Beta-Binomial).
- [[L2_5_Quiz#Question 9]] - Identifying common conjugate prior pairs.
- [[L2_5_Quiz#Question 15]] - Applying the Naive Bayes classifier.

## From Lecture 2.7: Maximum A Posteriori and Full Bayesian Inference
- [[L2_7_Quiz#Question 1]] - Calculating and comparing MAP and MLE estimates (Beta-Binomial).
- [[L2_7_Quiz#Question 2]] - Calculating MAP estimate and posterior predictive distribution for a Normal mean.
- [[L2_7_Quiz#Question 3]] - Performing Bayesian model comparison using marginal likelihood and Bayes factors.
- [[L2_7_Quiz#Question 4]] - Applying Bayesian inference to Linear Regression (MAP estimate, posterior predictive).
- [[L2_7_Quiz#Question 7]] - Interpreting Ridge and Lasso regression as MAP estimation with specific priors.
- [[L2_7_Quiz#Question 9]] - Applying Bayesian Model Averaging for prediction.
- [[L2_7_Quiz#Question 10]] - Using the Bayesian Information Criterion (BIC) for model selection.
- [[L2_7_Quiz#Question 20]] - Interpreting ML, MAP, and MMSE estimates from graphical representations of distributions.
- [[L2_7_Quiz#Question 29]] - Applying decision theory to minimize expected loss (Bayes risk) with different loss functions.
- [[L2_7_Quiz#Question 31]] - Using Bayes risk for decision making in a multi-class problem with asymmetric loss (medical diagnosis).
