# Machine Learning

Welcome to the unofficial course materials for Machine Learning. This knowledge base contains lecture notes, assignments, and supplementary resources to help you succeed in the course.

## Course Information

- **Instructor**: Dr. Zahra Rahimi
- **Prerequisites**: Probability, Linear Algebra, Programming

## Course Syllabus

### [[Lectures/1/L1_ML_Fundamentals|Lecture 1: Machine Learning Fundamentals]]
- Types of learning (Supervised, Unsupervised, Reinforcement, Semi-supervised)
- Pattern recognition and problem formulation
- Generalization concepts and bias-variance tradeoff
- Well-posed learning problems
- Required Reading: Chapter 1 of "Pattern Recognition and Machine Learning" by Bishop

### [[Lectures/2/L2_Probability_and_Statistical_Foundations|Lecture 2: Probability and Statistical Foundations for ML]]
- Probability fundamentals and distributions
- Information theory and entropy
- Statistical estimation and likelihood
- Maximum Likelihood Estimation (MLE)
- Bayesian inference and prior distributions
- Maximum A Posteriori (MAP) estimation
- Required Reading: Chapters 2 and 3.1-3.3 of "Pattern Recognition and Machine Learning" by Bishop

### [[Lectures/3/L3_Linear_Regression|Lecture 3: Linear Regression]]
- Mathematical foundations and linear algebra for linear models
- Simple linear regression fundamentals and implementation
- Probabilistic view and Maximum Likelihood in regression
- Multiple linear regression and feature engineering
- Optimization techniques (Gradient Descent, SGD)
- Model evaluation and validation methods
- Regularization techniques (Ridge, Lasso, Elastic Net)
- Advanced topics and real-world applications
- Required Reading: Chapter 3 of "Pattern Recognition and Machine Learning" by Bishop

### [[Lectures/4/L4_Linear_Classifiers|Lecture 4: Linear Classifiers]]
- The Perceptron algorithm
- Linear classification and decision boundaries
- Convergence analysis
- Multi-class classification
- Required Reading: Chapter 4.1-4.3 of "Pattern Recognition and Machine Learning" by Bishop

### [[Lectures/5/L5_Logistic_Regression|Lecture 5: Logistic Regression]]
- Binary classification with logistic function
- Multi-class logistic regression
- Maximum likelihood estimation for classification
- Newton's method and optimization
- Regularized logistic regression
- Comparison with other classifiers
- Required Reading: Chapter 4.3-4.4 of "Pattern Recognition and Machine Learning" by Bishop

### [[Lectures/6/L6_Support_Vector_Machines|Lecture 6: Support Vector Machines]]
- Maximum margin classifiers
- Hard margin and soft margin SVMs
- Kernel trick for nonlinear classification
- Multi-class SVM approaches
- SVM regression
- Computational considerations
- Required Reading: Chapter 7 of "Pattern Recognition and Machine Learning" by Bishop

## How to Use This Vault

1. Start by exploring the lecture materials in sequence
2. Complete readings before each lecture
3. Use the graph view to visualize relationships between concepts
4. Search for specific terms using Obsidian's search functionality
5. Add your own notes during lectures to enhance understanding

## Resources

- Course Textbook: "Pattern Recognition and Machine Learning" by Christopher Bishop
- Additional Resources: [Course Website]